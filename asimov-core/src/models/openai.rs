use std::sync::Arc;

use async_openai::{
    types::{
        ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs,
        CreateEmbeddingRequestArgs,
    },
    Client,
};
use async_stream::stream;
use async_trait::async_trait;
use futures::StreamExt;
use serde::{de::DeserializeOwned, Deserialize};
use typed_builder::TypedBuilder;

use crate::{
    error::Result,
    io::{Input, RawString, StreamedOutput},
    tokenizers::openai::OpenAiTiktoken,
    AsimovError,
};

use super::capabilities::{Embed, Generate};
use crate::io::{JsonStream, TokenStream};

#[derive(TypedBuilder, Clone)]
pub struct OpenAiLlm {
    #[builder(default = "gpt-3.5-turbo".to_string())]
    /// Model name. See the [OpenAI docs](https://platform.openai.com/docs/guides/text-generation)
    model: String,
    #[builder(default, setter(strip_option))]
    /// Stop after `max_tokens` tokens
    max_tokens: Option<u16>,
    #[builder(default, setter(strip_option))]
    /// Stopping criterion: stop generation upon detected sequence
    stop: Option<String>,
    #[builder(default, setter(strip_option))]
    /// Stopping criterion: stop generation upon detected sequence
    temperature: Option<f32>,
}

impl Default for OpenAiLlm {
    fn default() -> Self {
        Self {
            model: "gpt-3.5-turbo".to_string(),
            max_tokens: Default::default(),
            stop: Default::default(),
            temperature: Default::default(),
        }
    }
}

impl OpenAiLlm {
    /// Generate a request to the LLM
    fn request(&self, input: impl Input) -> Result<CreateChatCompletionRequestArgs> {
        let prompt = input.render()?;

        let message = ChatCompletionRequestUserMessageArgs::default()
            .content(prompt)
            .build()?
            .into();

        let mut request = CreateChatCompletionRequestArgs::default();

        request
            .model(&self.model.to_string())
            .n(1)
            .messages([message]);

        if let Some(stop) = &self.stop {
            request.stop(stop);
        }
        if let Some(max_tokens) = self.max_tokens {
            request.max_tokens(max_tokens);
        }
        if let Some(temperature) = self.temperature {
            request.temperature(temperature);
        }

        Ok(request)
    }

    /// Use the model to generate a `String` response.
    async fn raw_string(&self, input: impl Input) -> Result<String> {
        let client = Client::new();

        let request = self.request(input)?.build()?;

        let response = client.chat().create(request).await?;

        let result = response
            .choices
            .into_iter()
            .nth(0)
            .ok_or_else(|| AsimovError::Output("No choices returned from OpenAI".to_string()))?
            .message
            .content
            .ok_or_else(|| {
                AsimovError::Output("No content in the choice returned from OpenAI".to_string())
            })?;

        Ok(result)
    }

    /// Create a stream over the tokens generated by the LLM.
    /// This is the building block for streaming responses.
    async fn stream_tokens(&self, input: impl Input) -> Result<TokenStream> {
        let client = Client::new();
        let request = self.request(input)?.build()?;

        let mut stream = client.chat().create_stream(request).await?;

        let s = stream! {
            while let Some(chunk) = stream.next().await {
                let choice = chunk.unwrap().choices.into_iter().nth(0).unwrap();
                let content = choice.delta.content;

                if let Some(text) = content {
                    yield text
                }
            }
        };

        Ok(TokenStream::new(s))
    }
}

#[async_trait]
impl<S> Generate<S> for OpenAiLlm
where
    for<'a> S: Deserialize<'a>,
{
    /// Implementation for any `Deserialize` type.
    ///
    /// Note: if using `String` as the expected output,
    /// the routine will expect the llm's result to be
    /// surrounded by quotation marks.
    ///
    /// To generate "raw" strings, use the [`RawString`] type
    /// instead.
    async fn generate(&self, input: impl Input) -> Result<S> {
        let raw = self.raw_string(input).await?;
        let parsed = serde_json::from_str(&raw)?;
        Ok(parsed)
    }
}

#[async_trait]
impl Generate<RawString> for OpenAiLlm {
    /// Pass the output of the LLM directly.
    async fn generate(&self, input: impl Input) -> Result<RawString> {
        let raw = self.raw_string(input).await?;
        Ok(RawString::new(raw))
    }
}

#[async_trait]
impl Generate<TokenStream> for OpenAiLlm {
    /// Stream the tokens generated by the LLM directly.
    async fn generate(&self, input: impl Input) -> Result<TokenStream> {
        self.stream_tokens(input).await
    }
}

#[async_trait]
impl<D: DeserializeOwned + Send + 'static> Generate<StreamedOutput<D>> for OpenAiLlm {
    /// Use `json_stream` to stream any type that implements
    /// [`Deserialize`].
    async fn generate(&self, input: impl Input) -> Result<StreamedOutput<D>> {
        let stream = self.stream_tokens(input).await?;
        let stream = stream.map(|t| Ok(t.into_bytes()));
        let stream = JsonStream::<D>::new(Box::pin(stream));

        Ok(StreamedOutput::<D>::new(stream))
    }
}

#[async_trait]
impl Generate<StreamedOutput<RawString>> for OpenAiLlm {
    /// Generate a stream of tokens. Note that because of the fallibility
    /// of the parsing operation inherent to the streaming routine,
    /// the stream will yield a `Result` of [`RawString`].
    ///
    /// That's arguably awkward since there's no actual parsing going on â€“
    /// and probably not what you want.
    ///
    /// Consider using [`TokenStream`] as the result type, since it will
    /// stream tokens, as [`String`], directly.
    async fn generate(&self, input: impl Input) -> Result<StreamedOutput<RawString>> {
        let stream = self.stream_tokens(input).await?;
        let stream = StreamedOutput::<RawString>::new(stream.map(|t| Ok(RawString::new(t))));
        Ok(stream)
    }
}

/// Struct handling the interaction with OpenAI's API for embedding text.
#[derive(TypedBuilder, Clone)]
pub struct OpenAiEmbedding {
    #[builder(default = "text-embedding-ada-002".to_string())]
    model: String,
}

impl Default for OpenAiEmbedding {
    fn default() -> Self {
        Self {
            model: "text-embedding-ada-002".to_string(),
        }
    }
}

#[async_trait]
impl Embed for OpenAiEmbedding {
    type Tokenizer = OpenAiTiktoken;
    const DIM: u32 = 1536;

    /// Embed any type that implement the [`Input`] trait.
    async fn embed<I: Input + ?Sized>(&self, input: &I) -> Result<Vec<f32>> {
        let prompt = input.render()?;

        let client = Client::new();

        let request = CreateEmbeddingRequestArgs::default()
            .model(&self.model.to_string())
            .input(prompt)
            .build()?;

        let response = client.embeddings().create(request).await?;

        let embedding = response.data.into_iter().nth(0).unwrap();

        Ok(embedding.embedding)
    }
}

#[async_trait]
impl<T> Embed for Arc<T>
where
    T: Embed + Sync + Send,
{
    type Tokenizer = OpenAiTiktoken;
    const DIM: u32 = 1536;
    async fn embed<I: Input + ?Sized>(&self, input: &I) -> Result<Vec<f32>> {
        (**self).embed(input).await
    }
}

#[cfg(test)]
mod tests {

    use crate::{lines, prompt, tokenizers::Tokenizer};

    use super::*;

    #[tokio::test]
    async fn test_embedding() -> Result<()> {
        std::env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set");

        let ada002 = OpenAiEmbedding::default();
        let embedding = ada002.embed(&"This is a test".to_string()).await?;
        assert_eq!(embedding.len(), 1536);
        Ok(())
    }
    #[tokio::test]

    async fn test_string_generation() -> Result<()> {
        std::env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set");

        let gpt35 = OpenAiLlm::default();
        let response: RawString = gpt35.generate("How are you doing").await?;
        Ok(())
    }

    #[derive(Deserialize, Debug, PartialEq)]
    struct MyType {
        test: u8,
    }

    #[tokio::test]
    async fn test_json_generation() -> Result<()> {
        std::env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set");

        let gpt35 = OpenAiLlm::builder().temperature(0.0).build();
        let response: MyType = gpt35
            .generate(r#"Output the following, verbatim: {"test": 1}"#)
            .await?;
        assert_eq!(response, MyType { test: 1 });
        Ok(())
    }

    #[tokio::test]
    async fn test_stream() -> Result<()> {
        std::env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set");

        let gpt35 = OpenAiLlm::builder().build();
        let mut s: TokenStream = gpt35.generate("How are you doing").await?;

        while let Some(item) = s.next().await {
            println!("{item}");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_json_stream_generation() -> Result<()> {
        std::env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set");

        let gpt35 = OpenAiLlm::builder().temperature(0.0).build();
        let mut s: StreamedOutput<MyType> = gpt35
            .generate(
                r#"Output the following twice, separated by a new line, verbatim: {"test": 1}"#,
            )
            .await?;

        while let Some(item) = s.next().await {
            let item = item.unwrap();
            assert_eq!(item, MyType { test: 1 });
            println!("{item:?}");
        }

        Ok(())
    }

    #[tokio::test]
    async fn test_llm_chain() -> Result<()> {
        std::env::var("OPENAI_API_KEY").expect("OPENAI_API_KEY must be set");
        #[derive(Deserialize, Debug)]
        struct Sentiment {
            sentiment: bool,
        }

        // First LLM for sentiment analysis
        let sentiment_analysis_llm = OpenAiLlm::builder()
            .temperature(0.0)
            .model("gpt-3.5-turbo".to_string())
            .build();

        // Second LLM for summarizing sentiment
        let summarizing_llm = OpenAiLlm::builder()
            .model("gpt-3.5-turbo".to_string())
            .temperature(0.0)
            .build();

        let input = "The product is of high quality and the customer service is excellent.";

        let sentiment_prompt = prompt!(
            lines! {
                "Return the sentiment, true represents positive, false is negative.",
            "The output format should be like so: {\"sentiment\": true}",
            "Q: {{input}}\nA:" },
            input
        );

        let sentiment: Sentiment = sentiment_analysis_llm.generate(sentiment_prompt).await?;
        let sent: bool = sentiment.sentiment;
        assert_eq!(sent, true);

        let summary_prompt = prompt!(
            lines! {
                "The sentiment of the text {{input}} is {{sent}}.",
                "Summarize it"
            },
            input,
            sent
        );

        // Use the second LLM to summarize the sentiment
        let summary: RawString = summarizing_llm.generate(summary_prompt).await?;
        let tokenizer = OpenAiTiktoken::default();

        let encoded_summary = tokenizer.encode(&summary);
        println!("Generated {} tokens.", encoded_summary.len());

        assert!(encoded_summary.len() > 3);

        println!("Summary: {summary}");

        Ok(())
    }
}
